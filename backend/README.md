# Backend: PDF Document Analysis and Question Answering with AI

# This backend enables uploading PDF documents, analyzing and vectorizing them using embeddings, and answering questions via LLMs with context-augmented retrieval (RAG).

# Features
REST API built with FastAPI
PDF file upload and processing
Text segmentation (chunking) and metadata extraction
Embedding generation using OpenAI text-embedding-ada-002
In-memory chunk indexing using FAISS
Context retrieval based on user queries
Streaming responses generated by GPT-3.5-Turbo via Server-Sent Events (SSE)
Fully in-memory execution (no database required)
Requirements
Python 3.11 or higher


# Development Commands
# Install dependencies:
pip install -r requirements.txt

# START COMMAND 
uvicorn main:app --reload  



# Required environment variables (.env):


# Health Check Endpoint
# GET /health
Response:
{ "status": "ok" }


# Technical Details
The PDF is split into chunks of 500–800 characters with overlap.
Each chunk is embedded using OpenAI.
Similarity searches are performed in-memory using FAISS.
Responses are streamed using stream_openai_response, which leverages GPT-3.5-Turbo and tiktoken to estimate token usage and cost.
Core Technologies: FastAPI, Uvicorn, PyMuPDF, Sentence-Transformers, FAISS-CPU, OpenAI, Tiktoken

# backend/
├── main.py                 # FastAPI application entry point
├── routes/                 # API endpoints (chat, upload, health)
├── services/               # PDF parsing and vector indexing logic
├── utils/                  # LLM wrappers and in-memory storage
├── schemas/                # Custom data models and types
├── requirements.txt        # Project dependencies
└── README.md               # Backend documentation
